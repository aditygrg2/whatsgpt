{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cvxPKtEnn2co"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import nltk\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from gensim.models import KeyedVectors\n",
        "import tensorflow_datasets as tfds\n",
        "from keras.layers import Input, Dense, Dropout, Activation, Flatten, MultiHeadAttention, BatchNormalization, Softmax\n",
        "from keras.models import Model\n",
        "import time\n",
        "from keras.losses import BinaryCrossentropy\n",
        "from keras.utils import plot_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBa3lgr1n2cp"
      },
      "outputs": [],
      "source": [
        "files = os.listdir(\"/content/drive/MyDrive/To Delete/chats\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7s2yiQ5Nn2cp"
      },
      "outputs": [],
      "source": [
        "file_lengths = []\n",
        "\n",
        "for file in files:\n",
        "    with open(\"/content/drive/MyDrive/To Delete/chats/\" + file, \"r\") as f:\n",
        "        file_lengths.append(len(f.read()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 501
        },
        "id": "JX-g7QA-n2cp",
        "outputId": "edebebc6-b3e1-469b-a394-fe7019057aa2"
      },
      "outputs": [],
      "source": [
        "plt.hist(file_lengths)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGWjrXY-n2cp"
      },
      "source": [
        "Creating a single chat source."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j1WYWYBqn2cp"
      },
      "outputs": [],
      "source": [
        "with open(\"main.txt\", \"w\") as m:\n",
        "    m.write(\"\")\n",
        "\n",
        "for file in files:\n",
        "    with open(\"/content/drive/MyDrive/To Delete/chats/\" + file, \"r\") as f:\n",
        "        with open(\"main.txt\", \"a\") as m:\n",
        "            m.write(f.read())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ref1Qhven2cq"
      },
      "outputs": [],
      "source": [
        "texts_lined = []\n",
        "\n",
        "with open(\"main.txt\", \"r\") as m:\n",
        "    texts_lined = m.readlines()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXM8Czqcn2cq",
        "outputId": "4a9bff9e-53ce-4f64-9c0a-27a348a869d5"
      },
      "outputs": [],
      "source": [
        "len(texts_lined)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB6fi4dFn2cq"
      },
      "source": [
        "Data Cleaning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RoB7jb1nn2cq"
      },
      "outputs": [],
      "source": [
        "def merge_each_msg_to_chat(texts):\n",
        "    i = 0\n",
        "    ans = []\n",
        "\n",
        "    while(i < len(texts)):\n",
        "        if(texts[i].startswith(\"[\")):\n",
        "            txt = texts[i]\n",
        "            i += 1\n",
        "            while((i < len(texts)) and (not str(texts[i]).startswith(\"[\"))):\n",
        "                txt += \" \" + texts[i]\n",
        "                i += 1\n",
        "            ans.append(txt)\n",
        "\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE8V2eIan2cq",
        "outputId": "910076c7-5820-419f-ba1f-14df6422787a"
      },
      "outputs": [],
      "source": [
        "chats_merged = merge_each_msg_to_chat(texts_lined)\n",
        "\n",
        "print(\"Percentage of data merged\", len(chats_merged)/len(texts_lined))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBdw-wZhn2cq"
      },
      "source": [
        "For handling continous messages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUij8sOOn2cq"
      },
      "outputs": [],
      "source": [
        "from time import strptime, mktime\n",
        "from datetime import timedelta\n",
        "\n",
        "def chat_merger(texts, time_desired):\n",
        "    ans = []\n",
        "\n",
        "    i = 0\n",
        "    removed_chars = 0\n",
        "\n",
        "    while(i<len(texts)):\n",
        "        try:\n",
        "            name = texts[i][20:].split(\":\")[0].strip()\n",
        "            loop_text = texts[i]\n",
        "            loop_time = mktime(strptime(texts[i][0:20], \"[%d/%m/%y, %H:%M:%S]\"))\n",
        "\n",
        "            i += 1\n",
        "\n",
        "            while((i<len(texts)) and (name == texts[i][20:].split(\":\")[0].strip())):\n",
        "                time_new = mktime(strptime(texts[i][0:20], \"[%d/%m/%y, %H:%M:%S]\"))\n",
        "\n",
        "                if((time_new - loop_time) <= 60*time_desired):\n",
        "                    loop_text += \" \" + texts[i][20:].split(\":\")[1].strip()\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "                i += 1\n",
        "                loop_time = time_new\n",
        "\n",
        "            ans.append(loop_text)\n",
        "\n",
        "\n",
        "        except:\n",
        "            removed_chars += 1\n",
        "            i += 1\n",
        "\n",
        "\n",
        "    print(\"Removed\", removed_chars, \"invalid whatsapp texts\")\n",
        "    return ans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W381Hd_7n2cq",
        "outputId": "e6ecab7e-c92e-4158-a200-0977c2a97671"
      },
      "outputs": [],
      "source": [
        "merged_chats = chat_merger(chats_merged, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "gAkhzxp1n2cq",
        "outputId": "d78996d6-e3f0-4be0-f6ed-a0dc10dcb549"
      },
      "outputs": [],
      "source": [
        "merged_chats[93]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtuAcrTgn2cq",
        "outputId": "076f92a6-ac66-4d58-8531-64dd7634cf05"
      },
      "outputs": [],
      "source": [
        "print(\"Percentage of data retained after merging\", len(merged_chats)/len(texts_lined))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8AaIur0n2cq",
        "outputId": "75b3b3d5-114c-4c52-a967-f725625f6d7b"
      },
      "outputs": [],
      "source": [
        "print(len(texts_lined))\n",
        "print(len(merged_chats))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPx6rnSBn2cq",
        "outputId": "a14790f1-7f01-46c3-fabe-425167c90e85"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "import emoji\n",
        "\n",
        "def remove_only_emoji_text(text):\n",
        "    if(emoji.emoji_count(text) == len(text[20:].split(\":\")[1].strip())):\n",
        "        return False\n",
        "\n",
        "    return True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Wd-hJMon2cq",
        "outputId": "6fefc983-9bbb-4499-a1c3-3651b48ebbfd"
      },
      "outputs": [],
      "source": [
        "texts_emojis_removed = []\n",
        "\n",
        "for texts in tqdm(merged_chats):\n",
        "    if(remove_only_emoji_text(texts)):\n",
        "        texts_emojis_removed.append(texts)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ofs8-Bg2n2cr",
        "outputId": "32d35be7-5e74-46d6-d41a-743f79b1054c"
      },
      "outputs": [],
      "source": [
        "print(\"Percentage of chat messages retained\", len(texts_emojis_removed)/len(texts_lined))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "KTHXFBLzn2cr",
        "outputId": "bee72e03-69ff-4aea-a77f-bfd801ac26e3"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "lengths = [len(x[20:].split(\":\")[1].strip()) for x in texts_emojis_removed]\n",
        "sns.boxplot(lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "i7HDJjt8n2cr",
        "outputId": "230a6734-a46e-40af-8c4a-493aed6b6b73"
      },
      "outputs": [],
      "source": [
        "sns.kdeplot(lengths)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 453
        },
        "id": "EqJv6j1_n2cr",
        "outputId": "0f770efb-4e5d-4d43-f0cd-a3df5cee7af8"
      },
      "outputs": [],
      "source": [
        "sns.ecdfplot(lengths[0:int(0.99*len(lengths))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMTxRsRRn2cr",
        "outputId": "611a0169-541d-4792-f847-b64454d8086e"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import numpy as np\n",
        "\n",
        "lengths = sorted(lengths)\n",
        "for i in np.arange(0.90, 0.99, 0.01):\n",
        "    print(i, \"th percentile of the data is \", lengths[math.floor(i*len(lengths))])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TL5C41H_n2cr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "def remove_weirdos(texts):\n",
        "    preprocessed_text = []\n",
        "\n",
        "    print(\"Removing Weirdos from the text\")\n",
        "    for text in texts:\n",
        "        msg = str(text[20:].split(\":\")[1].strip().lower())\n",
        "\n",
        "        invalid_ct = [\"ommitted\", \"Messages and calls are end-to-end encrypted\", \"https\", \"http\", \"Missed voice call\", \"Missed video call\", \"deleted\", \"created group\", \".tech\", \".com\", \".in\"]\n",
        "\n",
        "        if(len(msg) > 80 or len(msg) < 2):\n",
        "            continue\n",
        "\n",
        "        temp = False\n",
        "\n",
        "        for i in invalid_ct:\n",
        "            if(msg.count(i)):\n",
        "                temp = True\n",
        "                break\n",
        "\n",
        "        if(temp):\n",
        "            continue\n",
        "\n",
        "        if(re.search(r\"@\\d+\", msg)):\n",
        "            continue\n",
        "\n",
        "        regex=r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "        if(re.search(regex, msg)):\n",
        "            continue\n",
        "\n",
        "        regex = r\"/^([a-zA-Z\\d\\-\\•])+@([\\w-])+\\•([a-z])+(\\•([a-z])+)?$\"\n",
        "        if(re.search(regex, msg)):\n",
        "            continue\n",
        "\n",
        "        msg = (msg.encode('ascii', 'ignore')).decode('utf-8').strip()\n",
        "\n",
        "        preprocessed_text.append(msg.replace(\"\\n\", \"\"))\n",
        "\n",
        "\n",
        "\n",
        "    print(\"Data Retained\", (len(preprocessed_text) / len(texts)) * 100, \"%\")\n",
        "\n",
        "    return preprocessed_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mrJpgX6Yn2cr",
        "outputId": "ee4abd59-f7f3-4bee-a99a-5b4cf6157098"
      },
      "outputs": [],
      "source": [
        "data_preprocessed = remove_weirdos(texts_emojis_removed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UGXcDb_An2cr"
      },
      "outputs": [],
      "source": [
        "words_tokenized = []\n",
        "\n",
        "for sentences in data_preprocessed:\n",
        "    words_tokenized.extend(sentences.split(\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSx0hxtfn2cr"
      },
      "outputs": [],
      "source": [
        "sorted_words_tokenized = sorted(words_tokenized)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4alCnPsyn2cr",
        "outputId": "7cf643f8-d555-4be7-9eff-31bb46f8b48e"
      },
      "outputs": [],
      "source": [
        "sorted_words_tokenized"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "copsNHrCn2cr",
        "outputId": "ea637834-daa0-4c13-949b-f2f576a2aaf2"
      },
      "outputs": [],
      "source": [
        "sorted_words_tokenized_preprocessed = []\n",
        "special_chars = ['*', '?', '%', \"`\", \"\\\\\",  '&', \"_\", \"<\", \">\",\"[\",\"]\", '$', '(', ')', '#', '^', '@', '!', '~', '-', '+', '=', \" \", \",\", \"'\", '\"',\"/\", \".\", \":\"]\n",
        "\n",
        "for words in tqdm(sorted_words_tokenized):\n",
        "    # This can be just made for a single word in future, because for tokenizing the values not all are required.\n",
        "    if(len(words) < 2):\n",
        "        continue\n",
        "\n",
        "    check_isalum = False\n",
        "\n",
        "    for char in words:\n",
        "        if char in special_chars or ord(char) > 127:\n",
        "            check_isalum = True\n",
        "            break\n",
        "\n",
        "    if(check_isalum):\n",
        "        continue\n",
        "\n",
        "    if(re.search(\"[0-9]\", words) and re.search(\"[a-zA-Z]\", words)):\n",
        "        continue\n",
        "\n",
        "    if(re.match(\"^[0-9]\", words)):\n",
        "        continue\n",
        "\n",
        "    if(emoji.emoji_count(words) == len(words)):\n",
        "        continue\n",
        "\n",
        "    if(len(set(words)) == 1):\n",
        "        continue\n",
        "\n",
        "    sorted_words_tokenized_preprocessed.append(words.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ac3p5tsmn2cr"
      },
      "outputs": [],
      "source": [
        "def token_checker(words):\n",
        "    # Tells if it is a invalid token or not\n",
        "    words = words.strip().lower()\n",
        "\n",
        "    for char in words:\n",
        "        if (char in special_chars or ord(char) > 127):\n",
        "            words = words.replace(char, \"\")\n",
        "            break\n",
        "\n",
        "    if(re.search(\"[0-9]\", words) and re.search(\"[a-z]\", words)):\n",
        "        return (True, \"\")\n",
        "\n",
        "    if(re.match(\"^[0-9]\", words)):\n",
        "        return (True, \"\")\n",
        "\n",
        "    if(emoji.emoji_count(words) == len(words)):\n",
        "        return (True, \"\")\n",
        "\n",
        "    if(len(words) < 2):\n",
        "        return (True, \"\")\n",
        "\n",
        "    if(len(set(words)) == 1):\n",
        "        return (True, \"\")\n",
        "\n",
        "    return (False, words)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTe586zgn2cr",
        "outputId": "50b84b23-8722-4839-a724-bdb8c7f69f3f"
      },
      "outputs": [],
      "source": [
        "uniq_sorted = list((sorted(set(sorted_words_tokenized_preprocessed))))\n",
        "\n",
        "print(len(uniq_sorted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EUVtYxEXn2cr"
      },
      "outputs": [],
      "source": [
        "mapper_english = {}\n",
        "mapper_hindi = {}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u3MAPMvYn2cr"
      },
      "outputs": [],
      "source": [
        "def word_enders(word):\n",
        "    test = word[-1]\n",
        "    first_test = False\n",
        "\n",
        "    for i in word[-3:]:\n",
        "        if(i!=test):\n",
        "            first_test = True\n",
        "            break\n",
        "\n",
        "    if(not first_test):\n",
        "        i = len(word) - 1\n",
        "        while(i > 0 and word[i] == test):\n",
        "            i -= 1\n",
        "\n",
        "        mapper_hindi[word] = word[0:i+2]\n",
        "        return word[0:i+2]\n",
        "\n",
        "    else:\n",
        "        return word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wMSSUUWkn2cs",
        "outputId": "2909e7a8-79de-46af-cc9a-faef8d662f0d"
      },
      "outputs": [],
      "source": [
        "uniq_sorted = [word_enders(word) for word in uniq_sorted]\n",
        "\n",
        "print(len(uniq_sorted))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W6bWmjoSn2cs"
      },
      "outputs": [],
      "source": [
        "with open(\"tokens_before.txt\", \"a\") as f:\n",
        "    for w in uniq_sorted:\n",
        "        f.write(w + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nvts5MO1n2cs"
      },
      "outputs": [],
      "source": [
        "def LCS_Match(first_str, second_str):\n",
        "    if(first_str[0:-1] == second_str[0:-1]):\n",
        "        if(\"\".join(sorted(first_str[-1] + second_str[-1])) == \"ae\"):\n",
        "            mapper_hindi[second_str] = first_str\n",
        "            return first_str\n",
        "\n",
        "        if(\"\".join(sorted(first_str[-1] + second_str[-1])) == \"ai\"):\n",
        "            mapper_hindi[second_str] = first_str\n",
        "            return first_str\n",
        "\n",
        "        if(\"\".join(sorted(first_str[-1] + second_str[-1])) == \"ie\"):\n",
        "            mapper_hindi[second_str] = first_str\n",
        "            return first_str\n",
        "\n",
        "    mini_str = first_str if len(first_str) < len(second_str) else second_str\n",
        "    other_str = second_str if len(first_str) < len(second_str) else first_str\n",
        "\n",
        "    if(abs(len(mini_str) - len(other_str))) == 1:\n",
        "        return mini_str\n",
        "\n",
        "    dp = [[0]*(len(other_str)+1) for i in range(len(mini_str)+1)]\n",
        "\n",
        "\n",
        "\n",
        "    for i in range(len(mini_str) - 1, -1, -1):\n",
        "        for j in range(len(other_str) - 1, -1, -1):\n",
        "            if(mini_str[i] == other_str[j]):\n",
        "                dp[i][j] = 1 + dp[i+1][j+1]\n",
        "            else:\n",
        "                dp[i][j] = max(dp[i+1][j], dp[i][j+1])\n",
        "\n",
        "    not_matched = \"\"\n",
        "    p = \"\"\n",
        "\n",
        "    normalizer = 1\n",
        "\n",
        "    if(len(mini_str) > 10):\n",
        "        normalizer = range(len(mini_str) - 2, len(mini_str) + 2)\n",
        "    elif(len(mini_str) > 5):\n",
        "        normalizer = range(len(mini_str) - 1, len(mini_str) + 1)\n",
        "    else:\n",
        "        normalizer = [len(mini_str)]\n",
        "\n",
        "\n",
        "    if(dp[0][0] in normalizer):\n",
        "        i = 0\n",
        "        j = 0\n",
        "\n",
        "        indexes = []\n",
        "\n",
        "        while((i<len(dp)) and (j<len(dp[0]))):\n",
        "            while((j+1<len(dp[0])) and dp[i][j+1] == dp[i][j]):\n",
        "                indexes.append(j)\n",
        "                j += 1\n",
        "\n",
        "            i += 1\n",
        "            j += 1\n",
        "\n",
        "        for i in range(0, len(other_str)):\n",
        "            if(i in indexes):\n",
        "                if(i+1 >= len(other_str)):\n",
        "                    p += other_str[i]\n",
        "                elif(other_str[i] != other_str[i+1]):\n",
        "                    p += other_str[i]\n",
        "            else:\n",
        "                p += other_str[i]\n",
        "\n",
        "        diff_length = abs(len(mini_str) - len(p))\n",
        "\n",
        "        if(diff_length < 2):\n",
        "            mapper_hindi[first_str] = p\n",
        "            mapper_hindi[second_str] = p\n",
        "            return p\n",
        "        else:\n",
        "            return \"\"\n",
        "\n",
        "    else:\n",
        "        return p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "nTWk5O55n2cs",
        "outputId": "9f97670d-2acd-4a5c-90f4-c7058ecf278e"
      },
      "outputs": [],
      "source": [
        "LCS_Match(\"ab\", \"abe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPzzyaJSn2cy"
      },
      "source": [
        "If this matches with the bigger string, it is not relevant to remove it. Also, if it returns none, means that the matching is not in proper range, so not feasible to merge/map the words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfqmw895n2cz",
        "outputId": "84180943-9332-4711-93a9-79a51d44ad7d"
      },
      "outputs": [],
      "source": [
        "len(uniq_sorted)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mkg4YnPn2cz"
      },
      "outputs": [],
      "source": [
        "def preprocess_epoch(data):\n",
        "    len_final_tokens = len(data)\n",
        "\n",
        "    i = 0\n",
        "    final_tokens = []\n",
        "\n",
        "    while(i+1 < len(data)):\n",
        "        result = LCS_Match(data[i], data[i+1])\n",
        "        if(len(result) > 0):\n",
        "            i += 2\n",
        "            final_tokens.append(result)\n",
        "        else:\n",
        "            final_tokens.append(data[i])\n",
        "            i += 1\n",
        "\n",
        "\n",
        "    final_tokens = [word_enders(word) for word in final_tokens]\n",
        "    print(\"Data retained\", len(final_tokens)/14033)\n",
        "\n",
        "    return final_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mBi-2Foun2cz",
        "outputId": "e0788d7d-5390-48c8-cd18-3a4a5e41ce33"
      },
      "outputs": [],
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('words')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import words as english_dictionary\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "\n",
        "vocab = set(english_dictionary.words())\n",
        "\n",
        "english_words = []\n",
        "\n",
        "for word in tqdm(uniq_sorted):\n",
        "    english_words.append(lemmatizer.lemmatize(word))\n",
        "\n",
        "for i, word in enumerate(english_words):\n",
        "    if(word in vocab):\n",
        "        mapper_english[uniq_sorted[i]] = word"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k3BNm9f6n2cz",
        "outputId": "c58eaf91-4f6d-4076-c600-0a4a369861ad"
      },
      "outputs": [],
      "source": [
        "mapper_created_data = uniq_sorted\n",
        "plot = []\n",
        "\n",
        "for i in range(0, 6):\n",
        "    mapper_created_data = preprocess_epoch(mapper_created_data)\n",
        "    plot.append(len(mapper_created_data) / 14033)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDr0mrV0n2cz"
      },
      "outputs": [],
      "source": [
        "with open(\"tokens.txt\", \"w\") as f:\n",
        "    # for word in data:\n",
        "    #     f.write(word + \"\\n\")\n",
        "    for i in mapper_hindi.items():\n",
        "        f.write(i[0] + \" : \" + i[1] + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RH490j9mn2cz"
      },
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "messages_improved = []\n",
        "\n",
        "def fact():\n",
        "    return 0\n",
        "\n",
        "tokens_improved = defaultdict(fact)\n",
        "from nltk.corpus import words\n",
        "\n",
        "for i in data_preprocessed:\n",
        "    data = i.split(\" \")\n",
        "    sentence = \"\"\n",
        "\n",
        "    for j in data:\n",
        "        check, result = token_checker(j)\n",
        "        # check tells if the token is invalid\n",
        "        if(check):\n",
        "            continue\n",
        "\n",
        "        if(result in mapper_english.keys()):\n",
        "            sentence += mapper_english[result] + \" \"\n",
        "            tokens_improved[str(mapper_english[result])] += 1\n",
        "        elif(result in mapper_hindi.keys()):\n",
        "            sentence += (str(mapper_hindi[result] + \" \"))\n",
        "            tokens_improved[str(mapper_hindi[result])] += 1\n",
        "        else:\n",
        "            sentence += (result + \" \")\n",
        "            tokens_improved[result] += 1\n",
        "\n",
        "    sentence = sentence.strip()\n",
        "    messages_improved.append(sentence)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZop4yjkn2cz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "tokens = pd.DataFrame({\n",
        "    \"token_string\": tokens_improved.keys(),\n",
        "    \"token_count\": tokens_improved.values(),\n",
        "})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 448
        },
        "id": "Ovz_dCgOn2cz",
        "outputId": "21db5146-1acd-4727-9125-70267e1e2e08"
      },
      "outputs": [],
      "source": [
        "tokens['token_count'].value_counts().plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z4shwgqan2cz"
      },
      "outputs": [],
      "source": [
        "less_occuring_tokens = set(tokens[tokens['token_count'] <= 4]['token_string'].values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53b5Nwwrn2cz",
        "outputId": "5ccf9550-65ff-4a8f-948b-93ac93ce1651"
      },
      "outputs": [],
      "source": [
        "messages_final = []\n",
        "\n",
        "for msg in tqdm(messages_improved):\n",
        "    if(len(msg) == 0):\n",
        "        continue\n",
        "\n",
        "    temp = msg.split(\" \")\n",
        "    intersected = less_occuring_tokens.intersection(temp)\n",
        "\n",
        "    text_str = \"\"\n",
        "\n",
        "    if(len(intersected) > 0):\n",
        "        for m in temp:\n",
        "            if(m not in intersected):\n",
        "                text_str += m + \" \"\n",
        "            else:\n",
        "                text_str += \"<UNK> \"\n",
        "    else:\n",
        "        text_str = msg\n",
        "\n",
        "    messages_final.append(text_str.strip())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q6vkVhjVn2cz"
      },
      "outputs": [],
      "source": [
        "maxl = 0\n",
        "\n",
        "for i, val in enumerate(messages_final):\n",
        "    messages_final[i] = val.split(\" \")\n",
        "    maxl = np.max([len(messages_final[i]), maxl])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E1JXEoxXn2cz"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "model = gensim.models.Word2Vec(sentences = messages_final, vector_size=128, min_count = 1, window = 5, sg=0)\n",
        "# Generates One Hot Encoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XX8pl0ran2cz",
        "outputId": "ad445c89-34df-47cd-ea56-af7213dd3708"
      },
      "outputs": [],
      "source": [
        "model.train(messages_final, epochs=100,total_examples = len(messages_final))\n",
        "model.save(\"word2vec_embeddings.model\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-80BFwsn2cz"
      },
      "outputs": [],
      "source": [
        "word_vectors = model.wv\n",
        "word_vectors.save(\"word2vec.wordvectors\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExQY_n2Pn2cz"
      },
      "outputs": [],
      "source": [
        "embeddings = KeyedVectors.load(\"word2vec.wordvectors\")\n",
        "key_to_embedding = dict(zip(embeddings.index_to_key, embeddings.vectors))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SYXAOwiyn2c0"
      },
      "outputs": [],
      "source": [
        "EMBEDDING_LENGTH = 128\n",
        "SEQ_LENGTH = 4\n",
        "BUFFER_SIZE = 3000\n",
        "BATCH_SIZE = 1\n",
        "NUM_HEADS = 8\n",
        "KEY_DIM = 64 #paper\n",
        "MHADROPOUT = 0.2\n",
        "WINDOW_SIZE = 4\n",
        "MAXL = 21\n",
        "EPOCHS = 20\n",
        "OPTIMIZER_LR = 0.0001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkJ7HZ09n2c0"
      },
      "outputs": [],
      "source": [
        "def embed_sentence_with_masking(seq):\n",
        "    values = []\n",
        "    for i in seq:\n",
        "        values.append(key_to_embedding[i])\n",
        "\n",
        "    return tf.convert_to_tensor(values)\n",
        "\n",
        "def positional_encoder(pos = SEQ_LENGTH, dmodel = EMBEDDING_LENGTH):\n",
        "    encoder = []\n",
        "\n",
        "    for p in range(pos):\n",
        "        temp = []\n",
        "        for i in range(0, dmodel):\n",
        "            if(i % 2 == 0):\n",
        "                temp.append(np.sin(p / np.power(10000, (2*i) // dmodel)))\n",
        "            else:\n",
        "                temp.append(np.cos(p / np.power(10000, (2*i) // dmodel)))\n",
        "\n",
        "        encoder.append(temp)\n",
        "\n",
        "    return np.array(encoder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "id": "MFyBlV34n2c0",
        "outputId": "8cc7ee2c-427b-4d39-a9c0-260bd3c73550"
      },
      "outputs": [],
      "source": [
        "positional_embeddings = positional_encoder()\n",
        "\n",
        "plt.pcolormesh(positional_embeddings, cmap='RdBu')\n",
        "plt.xlabel('Depth')\n",
        "plt.ylabel('Position')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jAx_hkadn2c0",
        "outputId": "b6b54dfa-0438-4945-bf75-b30d9b585a6a"
      },
      "outputs": [],
      "source": [
        "look_ahead_masks = []\n",
        "\n",
        "for i in range(0, SEQ_LENGTH-1):\n",
        "    look_ahead_masks.append([1]*(i+1) + [0] * (SEQ_LENGTH-1-i))\n",
        "\n",
        "len(look_ahead_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wRAYih78n2c0"
      },
      "outputs": [],
      "source": [
        "temp = []\n",
        "unique_word_set = dict()\n",
        "int_to_word_set = dict()\n",
        "\n",
        "ptr = 0\n",
        "\n",
        "for i in messages_final:\n",
        "    temp.extend(i)\n",
        "\n",
        "    for j in i:\n",
        "        if j not in unique_word_set.keys():\n",
        "            unique_word_set[j] = ptr\n",
        "            int_to_word_set[ptr] = j\n",
        "            ptr += 1\n",
        "\n",
        "temp = np.array(temp)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UoF85hQzn2c0"
      },
      "outputs": [],
      "source": [
        "vocab_length = len(unique_word_set)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEVLx_ZOn2c0"
      },
      "outputs": [],
      "source": [
        "x_tensor = []\n",
        "y_tensor = []\n",
        "\n",
        "for i, val in enumerate(temp):\n",
        "    if(i+WINDOW_SIZE < len(temp)):\n",
        "        x_tensor.append(temp[i:i+WINDOW_SIZE])\n",
        "        y_tensor.append(temp[i+WINDOW_SIZE])\n",
        "\n",
        "x_tensor = tf.convert_to_tensor(np.array(x_tensor))\n",
        "y_tensor = tf.convert_to_tensor(np.array(y_tensor))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSKkOQcYn2c0"
      },
      "outputs": [],
      "source": [
        "data = tf.data.Dataset.from_tensor_slices((x_tensor, y_tensor))\n",
        "\n",
        "test_data = data.take(int(0.1*len(data)))\n",
        "train_data = data.skip(int(0.1*len(data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZ6MM6Wzn2c0",
        "outputId": "9bbe7b6e-1aec-4193-a7c7-af8930b7cd00"
      },
      "outputs": [],
      "source": [
        "print(len(test_data))\n",
        "print(len(train_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y_-PvvS2n2c0"
      },
      "outputs": [],
      "source": [
        "def masked_binary_tensor(seq):\n",
        "    return tf.cast(tf.equal(seq, 0), dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gk9uedpUn2c0"
      },
      "outputs": [],
      "source": [
        "def make_batches(ds):\n",
        "    return ds.cache().batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgDdJYprn2c0"
      },
      "outputs": [],
      "source": [
        "test_data = make_batches(test_data)\n",
        "train_data = make_batches(train_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rKJuaZSn2c0"
      },
      "outputs": [],
      "source": [
        "def create_look_ahead_mask():\n",
        "    vec = 1 - tf.linalg.band_part(tf.ones((SEQ_LENGTH,SEQ_LENGTH)), -1, 0)\n",
        "    return vec"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SnckjpDVn2c0"
      },
      "source": [
        "200 sized embedding -> add with positional_embeddings -> iterate it 8 times ->  Predict the word, penalize with cross entropy (Ignore the ones with padding) -> Train decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSpOVku8n2c0"
      },
      "outputs": [],
      "source": [
        "class WhatsGPT(tf.keras.Model):\n",
        "    def __init__(self, num_heads, key_dim, value_dim, dropout, ff_layers, vocab_length):\n",
        "        super(WhatsGPT, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.key_dim = key_dim\n",
        "        self.value_dim = value_dim\n",
        "        self.dropout = dropout\n",
        "        self.ff_layers = ff_layers\n",
        "\n",
        "        self.mha = MultiHeadAttention(num_heads, key_dim, value_dim, dropout)\n",
        "        self.ff = Dense(ff_layers, activation='relu')\n",
        "\n",
        "        self.bn_1 = BatchNormalization()\n",
        "        self.bn_2 = BatchNormalization()\n",
        "\n",
        "        self.dense = Dense(vocab_length, activation='linear')\n",
        "\n",
        "        self.probabs = Softmax()\n",
        "\n",
        "    def call(self, train_data, look_ahead_mask, training):\n",
        "        attention_output = self.mha(train_data, train_data, train_data, look_ahead_mask, training=training) # (batch, seq_length, emb_size)\n",
        "\n",
        "        add_and_norm_1 = attention_output + train_data # (batch, seq_length, emb_size)\n",
        "\n",
        "        batch_normalized = self.bn_1(add_and_norm_1) # (batch, seq_length, emb_size)\n",
        "\n",
        "        feed_forward_output = self.ff(add_and_norm_1) # (batch, seq_length, emb_size)\n",
        "\n",
        "        add_and_norm_2 = batch_normalized + feed_forward_output # (batch, seq_length, emb_size)\n",
        "\n",
        "        batch_normalized_2 = self.bn_2(add_and_norm_2) # (batch, seq_length, emb_size)\n",
        "\n",
        "        final_output = self.dense(batch_normalized_2) # (batch, seq_length, emb_size)\n",
        "\n",
        "        final_output_probabs = self.probabs(final_output) # (batch, seq_length, emb_size)\n",
        "\n",
        "        return final_output_probabs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cae_9JIon2c0"
      },
      "outputs": [],
      "source": [
        "look_ahead_mask = create_look_ahead_mask()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qwFTYM1Dn2c0"
      },
      "outputs": [],
      "source": [
        "whatsgpt = WhatsGPT(NUM_HEADS, KEY_DIM, KEY_DIM, MHADROPOUT, ff_layers=EMBEDDING_LENGTH, vocab_length=len(unique_word_set))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBG5O6sGn2c0"
      },
      "outputs": [],
      "source": [
        "loss = BinaryCrossentropy(reduction=\"none\")\n",
        "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
        "train_accuracy = tf.keras.metrics.Mean(name='train_accuracy')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySNfDSjfn2c0"
      },
      "outputs": [],
      "source": [
        "def loss_function(pred, real):\n",
        "    return loss(real, pred)\n",
        "\n",
        "optimizer = tf.keras.optimizers.legacy.Adam(OPTIMIZER_LR, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FwiL7Y1Ln2c1"
      },
      "outputs": [],
      "source": [
        "def accuracy_calculator(real, pred):\n",
        "    accuracies = tf.cast(tf.equal(real, tf.cast(tf.argmax(pred, axis=2), tf.float32)), tf.int32)\n",
        "    return tf.reduce_sum(accuracies)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NA7nG7OOn2c1"
      },
      "outputs": [],
      "source": [
        "def create_one_hot_encoded_vectors(target):\n",
        "    ohe_vectors = []\n",
        "\n",
        "    for i in target:\n",
        "        ohe_vectors.append(tf.one_hot(unique_word_set[i], vocab_length))\n",
        "\n",
        "    return tf.convert_to_tensor(ohe_vectors, dtype=tf.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yGF0zfan2c1",
        "outputId": "323a79d7-fc96-4ab9-8b7c-576adfc8194f"
      },
      "outputs": [],
      "source": [
        "checkpoint_path = \"/content/drive/MyDrive/To Delete/whatsGPT LLM Checkpoints\"\n",
        "\n",
        "ckpt = tf.train.Checkpoint(whatsgpt)\n",
        "\n",
        "ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n",
        "\n",
        "# if a checkpoint exists, restore the latest checkpoint.\n",
        "if ckpt_manager.latest_checkpoint:\n",
        "  ckpt.restore(ckpt_manager.latest_checkpoint)\n",
        "  print ('Latest checkpoint restored!!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "id": "HZr39H7sn2c1",
        "outputId": "f5003dc7-5599-4093-b795-6a324257749e"
      },
      "outputs": [],
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    print(\"EPOCH: \", epoch)\n",
        "    start = time.time()\n",
        "\n",
        "    train_loss.reset_states()\n",
        "    train_accuracy.reset_states()\n",
        "\n",
        "    ckpt_save_path = ckpt_manager.save()\n",
        "    print (f'Saving checkpoint for epoch {epoch} at {ckpt_save_path}')\n",
        "\n",
        "    for (batch, (inp, tar)) in enumerate(train_data):\n",
        "        embeddings = inp.numpy().astype(type(''))[0]\n",
        "        target = np.concatenate((inp.numpy().astype(type(''))[0][1:], tar.numpy().astype(type(\"\"))))\n",
        "\n",
        "        embeddings = embed_sentence_with_masking(embeddings)\n",
        "        embeddings += positional_embeddings\n",
        "\n",
        "        final_vector = create_one_hot_encoded_vectors(target)\n",
        "\n",
        "        embeddings = embeddings[tf.newaxis, :, :]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = whatsgpt(embeddings, look_ahead_mask, True)\n",
        "\n",
        "            predictions = predictions[tf.newaxis, :, :]\n",
        "            loss_2 = loss_function(tf.transpose(predictions), final_vector)\n",
        "            gradients = tape.gradient(loss_2, whatsgpt.trainable_weights)\n",
        "\n",
        "            optimizer.apply_gradients(zip(gradients, whatsgpt.trainable_weights))\n",
        "\n",
        "            train_loss(loss_2)\n",
        "            train_accuracy(accuracy_calculator(final_vector, predictions)/WINDOW_SIZE)\n",
        "\n",
        "\n",
        "        if (batch + 1) % 50 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} Loss {train_loss.result():.4f} Accuracy {train_accuracy.result():.4f}')\n",
        "\n",
        "    end = time.time()\n",
        "    print(\"Time taken to complete:\", end - start)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gb4TxSv5n2c1"
      },
      "outputs": [],
      "source": [
        "whatsgpt.save_weights(\"model_tech_community_10.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5rNhPoWTn2c1",
        "outputId": "21aed06b-aa47-4e84-c19e-29ad2d5f1905"
      },
      "outputs": [],
      "source": [
        "text = [\"i\", \"love\", \"github\", \"do\", \"you\"]\n",
        "# TODO: Preprocess before entering into the prediction function\n",
        "\n",
        "i = len(text)\n",
        "\n",
        "while(i<12):\n",
        "    if len(text) > 4:\n",
        "      text = text[:-4]\n",
        "\n",
        "    embed = embed_sentence_with_masking(text)\n",
        "\n",
        "    embed = embed[tf.newaxis, :,:]\n",
        "\n",
        "    output = whatsgpt(embed, None, False)\n",
        "\n",
        "    text.append(int_to_word_set[tf.argmax(output[0][0]).numpy()])\n",
        "    print(int_to_word_set[tf.argmax(output[0][0]).numpy()])\n",
        "\n",
        "    i += 1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
